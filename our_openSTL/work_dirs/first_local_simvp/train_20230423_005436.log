2023-04-23 00:54:36,889 - Environment info:
------------------------------------------------------------
sys.platform: darwin
Python: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]
CUDA available: False
GCC: Apple clang version 14.0.3 (clang-1403.0.22.14.1)
PyTorch: 2.0.0
PyTorch compiling details: PyTorch built with:
  - GCC 4.2
  - C++ Version: 201703
  - clang 13.1.6
  - Intel(R) oneAPI Math Kernel Library Version 2022.1-Product Build 20220312 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201811
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: NO AVX
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/Applications/Xcode_13.3.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -Wno-deprecated-declarations -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_PYTORCH_METAL_EXPORT -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DUSE_COREML_DELEGATE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=braced-scalar-init -Werror=range-loop-construct -Werror=bool-operation -Winconsistent-missing-override -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wvla-extension -Wno-range-loop-analysis -Wno-pass-failed -Wsuggest-override -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wconstant-conversion -Wno-invalid-partial-specialization -Wno-typedef-redefinition -Wno-unused-private-field -Wno-inconsistent-missing-override -Wno-constexpr-not-const -Wno-missing-braces -Wunused-lambda-capture -Wunused-local-typedef -Qunused-arguments -fcolor-diagnostics -fdiagnostics-color=always -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -DUSE_MPS -fno-objc-arc -Wno-unguarded-availability-new -Wno-unused-private-field -Wno-missing-braces -Wno-constexpr-not-const, LAPACK_INFO=mkl, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.0.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.15.1
OpenCV: 4.7.0
openstl: 0.2.0
------------------------------------------------------------

2023-04-23 00:54:39,023 - 
device: 	cpu	
dist: 	False	
display_step: 	10	
res_dir: 	work_dirs	
ex_name: 	first_local_simvp	
use_gpu: 	False	
fp16: 	True	
torchscript: 	False	
seed: 	42	
diff_seed: 	False	
fps: 	False	
empty_cache: 	True	
find_unused_parameters: 	False	
resume_from: 	work_dirs/first_local_simvp/checkpoints/latest.pth	
auto_resume: 	True	
test: 	False	
deterministic: 	False	
launcher: 	none	
local_rank: 	0	
port: 	29500	
batch_size: 	16	
val_batch_size: 	1	
num_workers: 	4	
data_root: 	../Dataset_Student_V2	
dataname: 	clevrer	
pre_seq_length: 	11	
aft_seq_length: 	11	
total_length: 	22	
method: 	simvp	
config_file: 	configs/mmnist/simvp/SimVP_gSTA.py	
model_type: 	gSTA	
drop: 	0.0	
drop_path: 	0	
epoch: 	200	
log_step: 	1	
opt: 	adam	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.0	
clip_grad: 	None	
clip_mode: 	norm	
sched: 	onecycle	
lr: 	0.001	
lr_k_decay: 	1.0	
warmup_lr: 	1e-05	
min_lr: 	1e-06	
final_div_factor: 	10000.0	
warmup_epoch: 	5	
decay_epoch: 	100	
decay_rate: 	0.1	
filter_bias_and_bn: 	False	
spatio_kernel_enc: 	3	
spatio_kernel_dec: 	3	
hid_S: 	64	
hid_T: 	512	
N_T: 	8	
N_S: 	4	
in_shape: 	[11, 3, 160, 240]	
2023-04-23 00:54:41,377 - Model info:
SimVP_Model(
  (enc): Encoder(
    (enc): Sequential(
      (0): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (1): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (2): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (3): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
    )
  )
  (dec): Decoder(
    (dec): Sequential(
      (0): ConvSC(
        (conv): BasicConv2d(
          (conv): Sequential(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (1): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (2): ConvSC(
        (conv): BasicConv2d(
          (conv): Sequential(
            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
      (3): ConvSC(
        (conv): BasicConv2d(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
          (act): SiLU(inplace=True)
        )
      )
    )
    (readout): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (hid): MidMetaNet(
    (enc): Sequential(
      (0): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(704, 704, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(704, 704, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=704)
              (conv_spatial): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=704)
              (conv1): Conv2d(704, 1408, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(704, 704, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.010)
          (norm2): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(704, 5632, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(5632, 5632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5632)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(5632, 704, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (reduction): Conv2d(704, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (3): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.006)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (4): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.004)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (5): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.003)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (6): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): DropPath(drop_prob=0.001)
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (7): MetaBlock(
        (block): GASubBlock(
          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (attn): SpatialAttention(
            (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            (activation): GELU(approximate='none')
            (spatial_gating_unit): AttentionModule(
              (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
              (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
              (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            )
            (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (drop_path): Identity()
          (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): MixMlp(
            (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
            (dwconv): DWConv(
              (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
            )
            (act): GELU(approximate='none')
            (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (reduction): Conv2d(512, 704, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
| module                           | #parameters or shape   | #flops     |
|:---------------------------------|:-----------------------|:-----------|
| model                            | 48.568M                | 0.164T     |
|  enc.enc                         |  0.113M                |  9.7G      |
|   enc.enc.0.conv                 |   1.92K                |   0.865G   |
|    enc.enc.0.conv.conv           |    1.792K              |    0.73G   |
|    enc.enc.0.conv.norm           |    0.128K              |    0.135G  |
|   enc.enc.1.conv                 |   37.056K              |   3.927G   |
|    enc.enc.1.conv.conv           |    36.928K             |    3.893G  |
|    enc.enc.1.conv.norm           |    0.128K              |    33.792M |
|   enc.enc.2.conv                 |   37.056K              |   3.927G   |
|    enc.enc.2.conv.conv           |    36.928K             |    3.893G  |
|    enc.enc.2.conv.norm           |    0.128K              |    33.792M |
|   enc.enc.3.conv                 |   37.056K              |   0.982G   |
|    enc.enc.3.conv.conv           |    36.928K             |    0.973G  |
|    enc.enc.3.conv.norm           |    0.128K              |    8.448M  |
|  dec                             |  0.37M                 |  39.347G   |
|   dec.dec                        |   0.37M                |   39.266G  |
|    dec.dec.0.conv                |    0.148M              |    3.927G  |
|    dec.dec.1.conv                |    37.056K             |    3.927G  |
|    dec.dec.2.conv                |    0.148M              |    15.707G |
|    dec.dec.3.conv                |    37.056K             |    15.707G |
|   dec.readout                    |   0.195K               |   81.101M  |
|    dec.readout.weight            |    (3, 64, 1, 1)       |            |
|    dec.readout.bias              |    (3,)                |            |
|  hid.enc                         |  48.085M               |  0.115T    |
|   hid.enc.0                      |   10.396M              |   24.908G  |
|    hid.enc.0.block               |    10.036M             |    24.043G |
|    hid.enc.0.reduction           |    0.361M              |    0.865G  |
|   hid.enc.1.block                |   5.332M               |   12.767G  |
|    hid.enc.1.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.1.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.1.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.1.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.1.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.1.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.2.block                |   5.332M               |   12.767G  |
|    hid.enc.2.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.2.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.2.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.2.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.2.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.2.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.3.block                |   5.332M               |   12.767G  |
|    hid.enc.3.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.3.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.3.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.3.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.3.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.3.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.4.block                |   5.332M               |   12.767G  |
|    hid.enc.4.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.4.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.4.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.4.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.4.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.4.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.5.block                |   5.332M               |   12.767G  |
|    hid.enc.5.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.5.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.5.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.5.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.5.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.5.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.6.block                |   5.332M               |   12.767G  |
|    hid.enc.6.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.6.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.6.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.6.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.6.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.6.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.7                      |   5.694M               |   13.632G  |
|    hid.enc.7.block               |    5.332M              |    12.767G |
|    hid.enc.7.reduction           |    0.361M              |    0.865G  |
--------------------------------------------------------------------------------

2023-04-23 00:57:33,311 - 
val	 mse:6078.00537109375, mae:21265.15625
2023-04-23 00:57:33,326 - Intermediate result: 6078.00537109375  (Index 0)
2023-04-23 00:57:33,340 - Epoch: 1, Steps: 1 | Lr: 0.0000407 | Train Loss: 0.0831230 | Vali Loss: 0.0527605

2023-04-23 01:00:38,495 - 
val	 mse:4284.30224609375, mae:17649.7265625
2023-04-23 01:00:38,502 - Intermediate result: 4284.30224609375  (Index 1)
2023-04-23 01:00:38,517 - Epoch: 2, Steps: 1 | Lr: 0.0000427 | Train Loss: 0.0538266 | Vali Loss: 0.0371901

2023-04-23 01:03:50,779 - 
val	 mse:3259.374267578125, mae:15128.736328125
2023-04-23 01:03:50,784 - Intermediate result: 3259.374267578125  (Index 2)
2023-04-23 01:03:50,797 - Epoch: 3, Steps: 1 | Lr: 0.0000461 | Train Loss: 0.0386618 | Vali Loss: 0.0282932

2023-04-23 01:07:24,404 - 
val	 mse:2597.629150390625, mae:13433.4853515625
2023-04-23 01:07:24,410 - Intermediate result: 2597.629150390625  (Index 3)
2023-04-23 01:07:24,428 - Epoch: 4, Steps: 1 | Lr: 0.0000508 | Train Loss: 0.0299946 | Vali Loss: 0.0225489

2023-04-23 01:11:10,986 - 
val	 mse:2169.293212890625, mae:12196.4677734375
2023-04-23 01:11:10,991 - Intermediate result: 2169.293212890625  (Index 4)
2023-04-23 01:11:11,005 - Epoch: 5, Steps: 1 | Lr: 0.0000569 | Train Loss: 0.0239376 | Vali Loss: 0.0188307

2023-04-23 01:15:13,409 - 
val	 mse:1936.14111328125, mae:11480.875
2023-04-23 01:15:13,417 - Intermediate result: 1936.14111328125  (Index 5)
2023-04-23 01:15:13,436 - Epoch: 6, Steps: 1 | Lr: 0.0000643 | Train Loss: 0.0204999 | Vali Loss: 0.0168068

2023-04-23 01:19:00,832 - 
val	 mse:1873.8133544921875, mae:11351.9990234375
2023-04-23 01:19:00,845 - Intermediate result: 1873.8133544921875  (Index 6)
2023-04-23 01:19:00,861 - Epoch: 7, Steps: 1 | Lr: 0.0000730 | Train Loss: 0.0182202 | Vali Loss: 0.0162657

2023-04-23 01:22:01,626 - 
val	 mse:1906.3851318359375, mae:11522.9501953125
2023-04-23 01:22:01,630 - Intermediate result: 1906.3851318359375  (Index 7)
2023-04-23 01:22:01,645 - Epoch: 8, Steps: 1 | Lr: 0.0000829 | Train Loss: 0.0171001 | Vali Loss: 0.0165485

2023-04-23 01:25:22,370 - 
val	 mse:1925.51171875, mae:11657.0595703125
2023-04-23 01:25:22,374 - Intermediate result: 1925.51171875  (Index 8)
2023-04-23 01:25:22,391 - Epoch: 9, Steps: 1 | Lr: 0.0000941 | Train Loss: 0.0169753 | Vali Loss: 0.0167145

2023-04-23 01:28:16,959 - 
val	 mse:1861.121826171875, mae:11458.423828125
2023-04-23 01:28:16,965 - Intermediate result: 1861.121826171875  (Index 9)
2023-04-23 01:28:16,980 - Epoch: 10, Steps: 1 | Lr: 0.0001065 | Train Loss: 0.0169548 | Vali Loss: 0.0161556

2023-04-23 01:31:11,511 - 
val	 mse:1700.64794921875, mae:10811.48828125
2023-04-23 01:31:11,516 - Intermediate result: 1700.64794921875  (Index 10)
2023-04-23 01:31:11,529 - Epoch: 11, Steps: 1 | Lr: 0.0001200 | Train Loss: 0.0164065 | Vali Loss: 0.0147626

2023-04-23 01:33:58,272 - 
val	 mse:1474.6658935546875, mae:9832.837890625
2023-04-23 01:33:58,278 - Intermediate result: 1474.6658935546875  (Index 11)
2023-04-23 01:33:58,293 - Epoch: 12, Steps: 1 | Lr: 0.0001347 | Train Loss: 0.0146568 | Vali Loss: 0.0128009

2023-04-23 01:36:43,628 - 
val	 mse:1247.2860107421875, mae:8791.4697265625
2023-04-23 01:36:43,634 - Intermediate result: 1247.2860107421875  (Index 12)
2023-04-23 01:36:43,647 - Epoch: 13, Steps: 1 | Lr: 0.0001505 | Train Loss: 0.0127388 | Vali Loss: 0.0108271

2023-04-23 01:39:30,690 - 
val	 mse:1080.923095703125, mae:7961.94384765625
2023-04-23 01:39:30,695 - Intermediate result: 1080.923095703125  (Index 13)
2023-04-23 01:39:30,711 - Epoch: 14, Steps: 1 | Lr: 0.0001673 | Train Loss: 0.0108379 | Vali Loss: 0.0093830

2023-04-23 01:42:13,673 - 
val	 mse:1004.7224731445312, mae:7557.98876953125
2023-04-23 01:42:13,677 - Intermediate result: 1004.7224731445312  (Index 14)
2023-04-23 01:42:13,692 - Epoch: 15, Steps: 1 | Lr: 0.0001851 | Train Loss: 0.0090657 | Vali Loss: 0.0087215

2023-04-23 01:44:58,928 - 
val	 mse:991.5949096679688, mae:7572.3662109375
2023-04-23 01:44:58,934 - Intermediate result: 991.5949096679688  (Index 15)
2023-04-23 01:44:58,950 - Epoch: 16, Steps: 1 | Lr: 0.0002039 | Train Loss: 0.0083192 | Vali Loss: 0.0086076

2023-04-23 01:47:43,252 - 
val	 mse:988.718505859375, mae:7663.08642578125
2023-04-23 01:47:43,257 - Intermediate result: 988.718505859375  (Index 16)
2023-04-23 01:47:43,273 - Epoch: 17, Steps: 1 | Lr: 0.0002236 | Train Loss: 0.0081051 | Vali Loss: 0.0085826

2023-04-23 01:50:26,384 - 
val	 mse:974.1892700195312, mae:7670.197265625
2023-04-23 01:50:26,389 - Intermediate result: 974.1892700195312  (Index 17)
2023-04-23 01:50:26,406 - Epoch: 18, Steps: 1 | Lr: 0.0002441 | Train Loss: 0.0082779 | Vali Loss: 0.0084565

2023-04-23 01:53:16,796 - 
val	 mse:952.9283447265625, mae:7599.39892578125
2023-04-23 01:53:16,802 - Intermediate result: 952.9283447265625  (Index 18)
2023-04-23 01:53:16,817 - Epoch: 19, Steps: 1 | Lr: 0.0002654 | Train Loss: 0.0077816 | Vali Loss: 0.0082719

2023-04-23 01:56:00,141 - 
val	 mse:912.8905639648438, mae:7307.5546875
2023-04-23 01:56:00,146 - Intermediate result: 912.8905639648438  (Index 19)
2023-04-23 01:56:00,159 - Epoch: 20, Steps: 1 | Lr: 0.0002874 | Train Loss: 0.0076128 | Vali Loss: 0.0079244

2023-04-23 01:58:47,706 - 
val	 mse:826.9506225585938, mae:6657.24462890625
2023-04-23 01:58:47,711 - Intermediate result: 826.9506225585938  (Index 20)
2023-04-23 01:58:47,727 - Epoch: 21, Steps: 1 | Lr: 0.0003101 | Train Loss: 0.0073187 | Vali Loss: 0.0071784

2023-04-23 02:01:30,654 - 
val	 mse:721.73388671875, mae:5848.5927734375
2023-04-23 02:01:30,660 - Intermediate result: 721.73388671875  (Index 21)
2023-04-23 02:01:30,675 - Epoch: 22, Steps: 1 | Lr: 0.0003334 | Train Loss: 0.0066011 | Vali Loss: 0.0062651

2023-04-23 02:04:13,936 - 
val	 mse:668.6605834960938, mae:5412.796875
2023-04-23 02:04:13,941 - Intermediate result: 668.6605834960938  (Index 22)
2023-04-23 02:04:13,957 - Epoch: 23, Steps: 1 | Lr: 0.0003572 | Train Loss: 0.0057313 | Vali Loss: 0.0058043

2023-04-23 02:06:58,478 - 
val	 mse:677.36572265625, mae:5505.658203125
2023-04-23 02:06:58,484 - Intermediate result: 677.36572265625  (Index 23)
2023-04-23 02:06:58,499 - Epoch: 24, Steps: 1 | Lr: 0.0003814 | Train Loss: 0.0051660 | Vali Loss: 0.0058799

2023-04-23 02:09:52,028 - 
val	 mse:682.7532348632812, mae:5587.671875
2023-04-23 02:09:52,034 - Intermediate result: 682.7532348632812  (Index 24)
2023-04-23 02:09:52,050 - Epoch: 25, Steps: 1 | Lr: 0.0004061 | Train Loss: 0.0054536 | Vali Loss: 0.0059267

2023-04-23 02:12:57,136 - 
val	 mse:662.4861450195312, mae:5442.322265625
2023-04-23 02:12:57,141 - Intermediate result: 662.4861450195312  (Index 25)
2023-04-23 02:12:57,154 - Epoch: 26, Steps: 1 | Lr: 0.0004311 | Train Loss: 0.0055547 | Vali Loss: 0.0057507

2023-04-23 02:16:09,023 - 
val	 mse:646.4154663085938, mae:5350.43505859375
2023-04-23 02:16:09,028 - Intermediate result: 646.4154663085938  (Index 26)
2023-04-23 02:16:09,041 - Epoch: 27, Steps: 1 | Lr: 0.0004563 | Train Loss: 0.0054649 | Vali Loss: 0.0056112

2023-04-23 02:19:04,674 - 
val	 mse:626.4932861328125, mae:5236.267578125
2023-04-23 02:19:04,679 - Intermediate result: 626.4932861328125  (Index 27)
2023-04-23 02:19:04,694 - Epoch: 28, Steps: 1 | Lr: 0.0004817 | Train Loss: 0.0053053 | Vali Loss: 0.0054383

2023-04-23 02:22:01,368 - 
val	 mse:587.6767578125, mae:4805.90283203125
2023-04-23 02:22:01,374 - Intermediate result: 587.6767578125  (Index 28)
2023-04-23 02:22:01,390 - Epoch: 29, Steps: 1 | Lr: 0.0005072 | Train Loss: 0.0047773 | Vali Loss: 0.0051014

2023-04-23 02:24:53,878 - 
val	 mse:564.8779296875, mae:4638.853515625
2023-04-23 02:24:53,883 - Intermediate result: 564.8779296875  (Index 29)
2023-04-23 02:24:53,899 - Epoch: 30, Steps: 1 | Lr: 0.0005328 | Train Loss: 0.0046169 | Vali Loss: 0.0049035

